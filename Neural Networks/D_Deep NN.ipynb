{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b2d25-ae8d-451d-9ed0-3b98bc526252",
   "metadata": {},
   "source": [
    "# Building and Training a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d1ecc-71d2-4378-b5df-b4f3d59937f4",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1e5946-9620-40c3-ab69-5637f2524ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c358ac-2f23-4877-89d8-4105a2792ed8",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b32aef-feb9-4e92-b21b-f73b4de7e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the derivative of sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c44c6f-d542-4e65-864d-8c4179f25718",
   "metadata": {},
   "source": [
    "### Initialize parameters (weights and biases) for a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddc562-ac6d-4e7c-a536-0647ba345d22",
   "metadata": {},
   "source": [
    "- This function initializes the weights (W) and biases (b) for each layer of a neural network. \n",
    "- It takes a list layer_dims that specifies the number of units in each layer. \n",
    "- It uses random initialization for weights and sets biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe91b96-de80-435d-8a88-ecfc6cbef85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1  # Number of layers (excluding input layer)\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b53d4-1601-4938-8b7f-f384f8e4092f",
   "metadata": {},
   "source": [
    "### Forward propagation for a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac8741-e6f7-4753-9a06-5f62f3426b26",
   "metadata": {},
   "source": [
    "- This function computes the forward propagation for a single layer of the neural network. It takes the previous layer's activation A_prev, the weights W, biases b, and an activation function (sigmoid or relu).\n",
    "- It computes the linear transformation Z, applies the specified activation function, and returns the activation A along with a cache containing intermediate values for later use in backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77304da-af76-4b0d-9a2f-ac9195a5bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = np.maximum(0, Z)\n",
    "    \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f4e31-16b8-47a4-b31a-1b0819539cc9",
   "metadata": {},
   "source": [
    "### Forward propagation for the entire deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1837792f-e6f4-4b5d-bdec-b5408f36cc8d",
   "metadata": {},
   "source": [
    "- This function performs forward propagation for the entire deep neural network. \n",
    "- It takes the input X and the neural network's parameters, including weights and biases.\n",
    "- It iteratively computes forward propagation for each hidden layer using the forward_propagation function and stores intermediate values in the caches. The final output AL is the output of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c31b0f4-c5bd-4ac5-9b83-eb1d1acc0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # Number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_propagation(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = forward_propagation(A, parameters[f'W{L}'], parameters[f'b{L}'], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2af31-99a2-42b4-8fa3-2ddc8307e57a",
   "metadata": {},
   "source": [
    "### Compute the cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889092d5-d9e3-4016-addb-9a2476d52c86",
   "metadata": {},
   "source": [
    "- This function computes the cross-entropy loss between the predicted AL and the true labels Y for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6b4694-9fa0-44f7-ae83-c0c6006846e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1be4e-3da2-4125-b104-76cf25e9426f",
   "metadata": {},
   "source": [
    "### Backward propagation for a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc862f-6953-49e1-8044-ee0ce9414a2b",
   "metadata": {},
   "source": [
    "- This function computes backward propagation for a single layer. \n",
    "- It takes the gradient dA from the next layer, the cache containing intermediate values, and the activation function used in that layer.\n",
    "- It computes the gradients with respect to the parameters (dW and db) and the gradient with respect to the previous layer's activations dA_prev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55248024-103e-4d0f-b118-dee88488fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(dA, cache, activation):\n",
    "    A_prev, W, b, Z = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * sigmoid_derivative(Z)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = dA * (Z > 0)\n",
    "    \n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df585f4f-d05d-473f-98f5-19d71e40dab2",
   "metadata": {},
   "source": [
    "### Backward propagation for the entire deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f096f-5ab8-492f-8efa-530e4d782469",
   "metadata": {},
   "source": [
    "- This function performs backward propagation for the entire deep neural network. \n",
    "- It takes the final output AL, true labels Y, and the caches containing intermediate values from forward propagation.\n",
    "- It computes gradients for each layer using the backward_propagation function and stores them in the grads dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cba260-bb36-4e58-bc3b-ca898f710b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L - 1]\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = backward_propagation(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = backward_propagation(grads[f'dA{l+1}'], current_cache, activation=\"relu\")\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8e8fd-cf0e-4f0b-a5b7-480a7ca7f5d2",
   "metadata": {},
   "source": [
    "### Update parameters using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa915213-714a-440b-b3b4-a840da59287b",
   "metadata": {},
   "source": [
    "- This function updates the neural network's parameters (weights and biases) using gradient descent. \n",
    "- It takes the current parameters, gradients grads, and a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c132bcb-82ad-413f-b136-59882b22efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f'W{l}'] -= learning_rate * grads[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * grads[f'db{l}']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ad251-c14b-41f6-b78a-8caa3780d789",
   "metadata": {},
   "source": [
    "### Build and train a deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ad2f9-cb31-4f2b-a543-e81d6aa8e391",
   "metadata": {},
   "source": [
    "- This function builds and trains a deep neural network. \n",
    "- It takes input data X, true labels Y, the layer dimensions specified in layer_dims, learning rate, and the number of training iterations.\n",
    "- It initializes parameters, performs forward and backward propagation, updates parameters iteratively, and prints the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49923492-dd8d-4631-a674-521c6d687b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_network(X, Y, layer_dims, learning_rate, num_iterations):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = deep_forward_propagation(X, parameters)\n",
    "        loss = compute_loss(AL, Y)\n",
    "        grads = deep_backward_propagation(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a79cd-6bc7-4056-9619-109c2de060c6",
   "metadata": {},
   "source": [
    "### Make predictions using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18bd3c-ec59-44f9-82a5-565cf255a3ea",
   "metadata": {},
   "source": [
    "- This function makes predictions using the trained neural network. \n",
    "- It takes the parameters and input data X.\n",
    "- It applies the sigmoid activation function to the final output AL and converts the probabilities to binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584d8ac2-85d3-46c7-8e47-c4ddcc9bd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    AL, _ = deep_forward_propagation(X, parameters)\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c4bfd-93a7-4171-9907-5b391e584c72",
   "metadata": {},
   "source": [
    "### Example usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20324fe-8fbf-4305-a817-da326e64fd72",
   "metadata": {},
   "source": [
    "- This section provides an example of how to use the functions defined earlier to build, train, and make predictions using a deep neural network. \n",
    "- It generates sample data, specifies the network architecture, sets hyperparameters, trains the network, and makes predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68525452-886b-49d3-8b68-904477294833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.6931\n",
      "Iteration 100: Loss = 0.6931\n",
      "Iteration 200: Loss = 0.6931\n",
      "Iteration 300: Loss = 0.6931\n",
      "Iteration 400: Loss = 0.6931\n",
      "Iteration 500: Loss = 0.6931\n",
      "Iteration 600: Loss = 0.6931\n",
      "Iteration 700: Loss = 0.6931\n",
      "Iteration 800: Loss = 0.6931\n",
      "Iteration 900: Loss = 0.6931\n",
      "Iteration 1000: Loss = 0.6931\n",
      "Iteration 1100: Loss = 0.6931\n",
      "Iteration 1200: Loss = 0.6931\n",
      "Iteration 1300: Loss = 0.6931\n",
      "Iteration 1400: Loss = 0.6931\n",
      "Iteration 1500: Loss = 0.6931\n",
      "Iteration 1600: Loss = 0.6931\n",
      "Iteration 1700: Loss = 0.6931\n",
      "Iteration 1800: Loss = 0.6931\n",
      "Iteration 1900: Loss = 0.6931\n",
      "Predictions for new data:\n",
      "[[0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(2, 400)\n",
    "Y = ((X[0, :] ** 2 + X[1, :] ** 2) < 1).astype(int)\n",
    "\n",
    "# Reshape Y to be a 1D array\n",
    "Y = Y.reshape(1, -1)\n",
    "\n",
    "# Define the architecture of the deep neural network\n",
    "layer_dims = [2, 5, 5, 1]  # Input layer, hidden layers, output layer (2-5-5-1 architecture)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_iterations = 2000\n",
    "\n",
    "# Train the deep neural network\n",
    "trained_parameters = deep_neural_network(X, Y, layer_dims, learning_rate, num_iterations)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = np.array([[-0.8, -0.8], [0.8, 0.8]])\n",
    "predictions = predict(trained_parameters, new_data)\n",
    "\n",
    "print(\"Predictions for new data:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa1dea-87ce-44a6-88ed-13876062185b",
   "metadata": {},
   "source": [
    "- In this case, new_data contains two data points:\n",
    "- [-0.8, -0.8] is predicted as 0, indicating a negative class.\n",
    "- [0.8, 0.8] is predicted as 1, indicating a positive class.\n",
    "- These predictions are based on the learned model's decision boundary, where values closer to 0 are classified as the negative class, and values closer to 1 are classified as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f1ecb-46e4-4633-8abc-be7ae59f50d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab765e-cb2e-42c4-962b-cda8bfd3b703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
