{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caa946e-21e2-4c24-a1ff-71d0ce8f6453",
   "metadata": {},
   "source": [
    "# Neural Networks example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7663446-372d-4742-81df-577432b7ea95",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cc81db-2cf5-4e8d-84dd-33ba622ffc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec04865-5c5d-447f-8e5b-f148355a70c3",
   "metadata": {},
   "source": [
    "### Define the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a71738-c7e6-4e8d-825a-6c58ac054550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65ec6d-f887-4232-94f5-999ebcef3216",
   "metadata": {},
   "source": [
    "### Define the derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcc83f0-4494-4d3e-8d16-d7f6aab87947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7350765-925d-4e89-9f94-714fbcf35378",
   "metadata": {},
   "source": [
    "### Initialize parameters (weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc5f9e-abb6-44f7-905e-963b94c151a6",
   "metadata": {},
   "source": [
    "- Initialize the weights (W1 and W2) and biases (b1 and b2) for the neural network.\n",
    "- These parameters are initialized with small random values to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37099ff4-5524-4ae2-9df5-95dbaca589e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec180b3b-1fd6-4dfb-825c-87279c5a968e",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188567f-fdc8-4196-8e12-62bfd2670be5",
   "metadata": {},
   "source": [
    "- Perform forward propagation through the neural network.\n",
    "- Calculate the activations (A1 and A2) in the hidden and output layers.\n",
    "- Use the tanh activation function for the hidden layer and sigmoid for the output layer.\n",
    "- Store intermediate values (Z1, A1, Z2, A2) in a cache for later use in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315f6092-4d38-4648-a72b-f0cef434ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)  # Corrected activation function\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325eb5cd-1738-441c-b863-d2ffe3fb1316",
   "metadata": {},
   "source": [
    "### Compute the cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58194398-7e47-4657-b4f3-dd0587114cd6",
   "metadata": {},
   "source": [
    "- Compute the cross-entropy loss between the predicted values (A2) and the true labels (Y).\n",
    "- This loss measures how well the network is performing and is used to update the parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3789f28-5124-49d6-a292-c7c148284b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55c987-7a20-4fee-91db-a6a7f4294dba",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db877ae-503c-4b30-ad33-fa57361ae7af",
   "metadata": {},
   "source": [
    "- Perform backward propagation to compute gradients.\n",
    "- Calculate gradients of the cost function with respect to the parameters (dW1, db1, dW2, db2) using the chain rule.\n",
    "- The gradients indicate how much each parameter should be adjusted to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb1101d-1699-4012-9c3d-55e1ce717b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))  # Corrected derivative\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd38ad-b44a-4692-824b-932ad8ef4423",
   "metadata": {},
   "source": [
    "### Update parameters using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcad7a6-1cfa-4312-a778-8f61a7961922",
   "metadata": {},
   "source": [
    "- Update the parameters (W1, b1, W2, b2) using gradient descent.\n",
    "- Adjust the parameters in the direction that reduces the loss.\n",
    "- The learning rate (learning_rate) controls the step size for updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db924ab-d7f1-493c-955a-ebf0c5c49b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = gradients['dW1']\n",
    "    db1 = gradients['db1']\n",
    "    dW2 = gradients['dW2']\n",
    "    db2 = gradients['db2']\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    updated_parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return updated_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53191c2a-caaa-4edd-a386-450d102c6ec0",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a09d1-66a0-4e24-8af6-6c28d7ed6e5d",
   "metadata": {},
   "source": [
    "- Initialize parameters and hyperparameters (input size, hidden size, output size, number of iterations, and learning rate).\n",
    "- Iterate through training for a specified number of iterations.\n",
    "- In each iteration, perform forward and backward propagation, compute the loss, and update parameters.\n",
    "- Print the loss at regular intervals to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d046b01-49a9-43ab-a8af-a81b7003b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, Y, hidden_size, num_iterations, learning_rate):\n",
    "    input_size = X.shape[0]\n",
    "    output_size = Y.shape[0]\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        gradients = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82fbc9-06ee-4ca6-817b-8ea18e50782c",
   "metadata": {},
   "source": [
    "### Make predictions using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697efe4e-4963-4054-ae53-66426f43e6e0",
   "metadata": {},
   "source": [
    "- Use the trained model to make predictions on new data.\n",
    "- Apply the forward propagation to the new data to get predictions.\n",
    "- Classify predictions based on a threshold of 0.5 (sigmoid output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "886511ff-dc89-4996-8ead-c4303990eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7edef-2e0b-44e2-82ea-f147a88bad55",
   "metadata": {},
   "source": [
    "### Generate sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758f920-2a5e-440c-b037-b06e2917c5f9",
   "metadata": {},
   "source": [
    "- Generate synthetic data (X and Y) for a 2-class classification problem.\n",
    "- The data consists of 2 features (X[0] and X[1]) and binary labels (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f6c0e2-3066-4fe4-bc20-a7807fa1b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(2, 400)\n",
    "Y = ((X[0, :] ** 2 + X[1, :] ** 2) < 1).astype(int)\n",
    "Y = Y.reshape(1, -1)  # Reshape Y to be a 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cb83c-9c31-4ed9-8d30-9b382457b58a",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2465203-f45e-41f0-bc58-71b989586363",
   "metadata": {},
   "source": [
    "- Set the hyperparameters (hidden size, number of iterations, and learning rate).\n",
    "- Call train_neural_network to train the neural network using the generated data.\n",
    "- The network learns to classify points inside a circle as 1 and points outside as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fea9347e-6ad0-42b3-b58c-b1fdb27918f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.6932\n",
      "Iteration 100: Loss = 0.6730\n",
      "Iteration 200: Loss = 0.5470\n",
      "Iteration 300: Loss = 0.2143\n",
      "Iteration 400: Loss = 0.1451\n",
      "Iteration 500: Loss = 0.1251\n",
      "Iteration 600: Loss = 0.1126\n",
      "Iteration 700: Loss = 0.1033\n",
      "Iteration 800: Loss = 0.0961\n",
      "Iteration 900: Loss = 0.0903\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 4\n",
    "num_iterations = 1000\n",
    "learning_rate = 1.2\n",
    "trained_parameters = train_neural_network(X, Y, hidden_size, num_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67ff4b-2f95-4cd3-a5f7-7d5553ffbf6f",
   "metadata": {},
   "source": [
    "### Make predictions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e522c-818e-4b28-b32e-3843c0a9274c",
   "metadata": {},
   "source": [
    "- Create a new dataset (new_data) for making predictions.\n",
    "- Call the predict function to classify the new data points based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c5a7b0-2610-433e-9f06-c6f6b54d55cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for new data:\n",
      "[[0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[-0.8, -0.8], [0.8, 0.8]])\n",
    "predictions = predict(trained_parameters, new_data)\n",
    "\n",
    "print(\"Predictions for new data:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84a39a-884f-49da-a6e5-10542fecc95e",
   "metadata": {},
   "source": [
    "- [0 0]: This means that both of the new data points were predicted to belong to class 0.\n",
    "- 0 usually represents the negative class or \"no\" class\n",
    "- So, in your case, the neural network predicted that both of the new data points do not belong to the positive class (class 1) and are assigned to the negative class (class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b0acf-59c4-400c-b362-bd980cb1a043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
