{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f672f4-acd6-4faf-afda-8d4f8d1a53a5",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6c0c8-b903-4f7f-bcdc-91d7151380b2",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28de8106-6971-4f79-9c50-773f43601c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837c17f-0f70-4fe2-8d6d-3b795418c8d5",
   "metadata": {},
   "source": [
    "### Define sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d144149-9b05-441e-a6f4-4b16288b25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51d111-eedf-4e6b-96b5-fec07a622703",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9a1081-a082-4fbf-aca1-672e3dadb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a838a3-144e-4bdd-a5b3-c73febac32ab",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e9a1-3087-4411-8827-52315b91ad53",
   "metadata": {},
   "source": [
    "- forward_propagation computes the linear transformation z and applies the sigmoid activation function to obtain the output A.\n",
    "- This represents the prediction of the model based on the current parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb705927-bd5f-4cbf-845c-ce30c9346844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, w, b):\n",
    "    z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51608bd9-12ab-468f-b1a6-8913500edab9",
   "metadata": {},
   "source": [
    "### Compute cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c690c-0faa-4ec1-83b9-5d251b9d077c",
   "metadata": {},
   "source": [
    "- The compute_cost function calculates the logistic loss between the predicted A and the true labels Y.\n",
    "- It measures how well the model is performing and quantifies the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097e078d-d638-4eff-bde6-64e9a82ac17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a3058-42c7-4f54-b01b-6f9cdc56586e",
   "metadata": {},
   "source": [
    "### Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49f182-64af-4f10-9912-1afe58e3c4fe",
   "metadata": {},
   "source": [
    "- backward_propagation computes the gradients of the cost function with respect to the parameters.\n",
    "- These gradients indicate the direction and magnitude of changes needed to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28f87b5-22d7-422a-ae94-a66c4216f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, A, Y):\n",
    "    m = Y.shape[1]\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2391c4-51ca-4997-b309-dc95008c6977",
   "metadata": {},
   "source": [
    "### Gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91abc4b-937b-4e26-ae04-0f88e25fc29d",
   "metadata": {},
   "source": [
    "- The optimize function uses gradient descent to update the parameters (w and b) iteratively.\n",
    "- It adjusts the parameters in the direction that reduces the cost, moving the model closer to the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c73c22-949f-48c3-8112-819439fb7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        A = forward_propagation(X, w, b)\n",
    "        cost = compute_cost(A, Y)\n",
    "        dw, db = backward_propagation(X, A, Y)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Iteration {i}: Cost = {cost}\")\n",
    "    \n",
    "    return w, b, costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170f4e0-e6bf-4730-801b-16f7d8e1c157",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ac4f4-509b-4d3b-bc15-0e6db62777bb",
   "metadata": {},
   "source": [
    "- The predict function uses the trained parameters to make predictions on new data.\n",
    "- It applies the model to classify new data points based on the threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1686e057-b873-40dc-bf6e-ec2bcb5304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    A = forward_propagation(X, w, b)\n",
    "    predictions = (A > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e672fa-c3be-4628-a74c-354b2bfe27ba",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c5ee4-eef6-4eb7-9486-55303ad0a227",
   "metadata": {},
   "source": [
    "- The main function generates synthetic data, initializes parameters, trains the model using gradient descent, and makes predictions on new data. \n",
    "- This code serves as a basic example of logistic regression and demonstrates key concepts in building a shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24951ca-b518-48d7-a41c-beb17c6eedcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 0.6931471805599453\n",
      "Iteration 100: Cost = 0.6885997243889159\n",
      "Iteration 200: Cost = 0.6858045429272811\n",
      "Iteration 300: Cost = 0.6840768634495958\n",
      "Iteration 400: Cost = 0.6830027361744655\n",
      "Iteration 500: Cost = 0.6823312200423725\n",
      "Iteration 600: Cost = 0.6819093071762642\n",
      "Iteration 700: Cost = 0.681643063700047\n",
      "Iteration 800: Cost = 0.6814744258897278\n",
      "Iteration 900: Cost = 0.6813672730301278\n",
      "Predictions for new data: [[1]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Generate some sample data\n",
    "    np.random.seed(0)\n",
    "    m = 100  # Number of examples\n",
    "    n = 2    # Number of features\n",
    "    X = np.random.randn(n, m)\n",
    "    Y = (np.random.rand(1, m) < 0.5).astype(int)  # Binary classification problem\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w, b = initialize_parameters(n)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    num_iterations = 1000\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Train the model\n",
    "    w, b, costs = optimize(w, b, X, Y, num_iterations, learning_rate)\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    new_data = np.array([[1.2, 0.5]])\n",
    "    predictions = predict(w, b, new_data.T)\n",
    "    print(f\"Predictions for new data: {predictions}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94206b5-d998-414a-b40d-70bbc712fad4",
   "metadata": {},
   "source": [
    "- The output shows the cost (error) at various iterations during training, starting from iteration 0 and going up to iteration 900.\n",
    "- As the training progresses, the cost decreases, indicating that the model is learning to make better predictions.\n",
    "- Finally, the predictions for the new data point [1.2, 0.5] are displayed as [[1]], which means the model predicts a class label of 1 for this data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bce47-ae63-4f9b-acec-625a9af67a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
